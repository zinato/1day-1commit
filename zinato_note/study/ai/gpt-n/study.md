# GPT-3

### What is GPT-3 ??

- Generative Pre-trained Transformer 3 (GPT - 3) 은 딥러닝을 이용해 인간다운 텍스트를 만들어내는 자기 회귀 언어
모델이다. 
- openAI 사가 만든 GPT-n 시리즈의 3세대 언어 예측 모델 (LLM)
- '많이 학습시키면 된다'라는 가정을 입증한 모델, 실제로 GPT-2와 유사한 모델을 방대한 양으로 학습 시킨 것일 뿐 

### What Can GPT-3 Do ?

- 분류 : 가장 기본, 어떤 문장에 긍정, 중립, 부정을 판단
- 대화 : 심심이, 이루다2와 같은 대화 서비스, 더 자연스러움
- 번역 : 구글 번역기보다는 수준은 떨어지나 AI 로서 번역을 한다는 것이 중요 
- 이모티콘으로 변환 : 해당 단어의 이미지를 이모티콘으로 표현 
- 요약 : 몇 백줄 되는 문장도 최대한 압축시켜 요약, 내용이 부실한 경우 추가까지 함
- 완성 : 내용이 부실한 경우 부족한 내용을 대신 써줌, 두개의 스토리 라인을 연결해 줄 수도 있음
- 응답 (Q&A) : 질문에 답변을 함, 퀴즈 스타일, 틀린 답변도 맞는 것처럼 당당히 얘기함,    
- 텍스트 삽입 : 문맥에 맞게 생략된 부분을 유추하여 집어 넣어 줌
- 텍스트 생성 : GPT-3의 가장 흥미있는 부분 중 하나, 단편소설, 라이트노벨, 영화 등 긴 형식의 텍스트를 생성할 수 있음
- 생략된 주어 삽입 : 생략이 되어 있는 화자의 인칭 대명사를 집어 넣어 줌 
- 커스텀마이징 API : 코딩 조건에 맞는 코딩을 해줌 (Link : [Github의 Copilot](https://github.com/features/copilot)

### 한계

> "GPT-3은 너무 과대평가되었습니다. 여러 칭찬은 감사하지만, 여전히 약점이 있고 이상한 실수를 하기도 합니다. AI가 세상을 바꿀 것이지만 GPT-3가 그 첫 발을 내딛은 것뿐이라 생각합니다. 여전히 알아낼 게 많아요." - 샘 알트만

- 효율성이 너무 떨어짐
- 사전 학습에 필요한 비용과 시간이 다소 방대하고 활용하기 쉽지 않음
- 현실 세계의 물리적 상식을 잘 모름 
  - 인간의 뇌처럼 '개념을 학습'하는 것이 아니라 뒤에 나올 가장 자연스러운 단어를 학습 하는 모델
- 학습에 사용된 예제를 외우고 패턴을 분석, 학습 하는 것이지 실제로 추론해내는 것이 아니다
- 새로운 정보 수용이 어려움, '**기억력**'이 없다

### GPT-4 에서 기대되는 부분 

- 아직 많은 부분이 공개 되지 않음

#### 모델 사이즈가 크게 변하지 않을 것

- GPT-3에 비해 크게 변하지 않을  (1750억개의 매개변수 예상)
- 모델 크기보다 모델 성능을 개선하는데 초점을 둘 것으로 보임

#### 하이퍼 파라미터 향상

- GPT-3는 비용 이슈로 충분히 훈련되지 않음
- 2022/3 OpenAI, Microsoft는 하이퍼파라미터를 조정하여 GPT-3 성능 개선을 확인함

#### 사람의 피드백을 통해 Alignment 향상

- AI Aligment는 최근 용어
- OpenAI는 2022년 초 InstructGPT를 릴리즈
- InstructGPT는 사람의 피드백과 강화 학습을 통해 성능 향상
- 
